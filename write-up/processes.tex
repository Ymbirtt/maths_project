\section{Markov Chains}

\subsection{A Formal Definition}

A stochastic process \cite[p590]{doob96} is a collection $\{X_t : t \in T\}$ (sometimes $X(t)$ for continuous $T$) of random variables. These collections may be indexed arbitrarily, but tend to be used to describe the evolution of some random series of events, using $T$ to be some representation of either discrete or continuous time, for instance $X_t$ may be the number of observed emissions from a radioactive source after $t$ minutes or the licence plate of the $t^{th}$ car to go past a speed camera.

A Discrete Time Markov Chain (DTMC) is a particular type of discrete-time stochastic process - one which obeys the Markov Property. In these, $T$ is set to $\{0,1,2,...\}=\mathbb{N}$ and we say that $(X_t)_{t \in \mathbb{N}}$ obeys the Markov Property iff

$$
\forall t \in T, s \in S \quad \pr (X_{t+1} = s | X_0, X_1, ... X_t) = \pr (X_{t+1} = s | X_t)\cite{mwmarkov}
$$

We refer to $S$ as the state-space, each $s \in S$ is a state, and $X_t$ represents the state at time t. The Markov Property essentially states ``given the present, the future is conditionally independent of the past" \cite{mwmarkov}. There is also a Continuous Time Markov Chain (CTMC, sometimes CTMP for Continuous Time Markov Process), fow which we set $T=\mathbb{R}^{+}$, let $\varepsilon$ be some positive value close to 0, and define a similar Markov Property;

$$
\forall s \in S \quad \pr (X(t+\varepsilon) = s | \{X(\tau) : \tau \leqslant t\}) = \pr (X(t+\varepsilon) = s | X(t))
$$

We'll be dealing exclusively with the discrete state-space case in this project, i.e. a Markov Chain where $S$ is discrete, though we'll need to use both continuous and discrete times. It is popular for many authors to set $S \subseteq \mathbb{Z}$, though our states can be integers, real numbers, popes, or any other completely arbitrary non-empty set. Where $S$ and $T$ are discrete, we define $\bm{\delta} = (\delta_i)_{i \in S}$, the initial probability vector, and $\Pi = (\pi_{ij})_{(i,j) \in S^2}$, the matrix of transition probabilities, such that,

\begin{align*}
\delta_i &= \pr (X_0 = i) \\
 \forall t \in \mathbb{N} \quad \pi_{ij} &= \pr(X_{t+1} = j | X_t = i)
\end{align*}

In this case, $\Pi$ is constant does not depend on time, a property referred to as homogeneity. Every homogeneous DTMC can be uniquley defined by the triple $(S,\bm{\delta},\Pi)$. Naturally, the row sums of $\Pi$ should be $1$, i.e.

$$
\forall i \in S, \; \sum_{j \in S} \pi_{ij} = 1
$$

For CTMCs, rather than a transition probability matrix, a transition rate matrix $Q = (q_{ij})_{(i,j) \in S^2}$ is defined for small $\varepsilon$ as;
$$
\forall i,j \in S, \quad \pr (X(t+\varepsilon)=j|X(t)=i) = q_{ij}\varepsilon + o(\varepsilon),
$$
where $o(\varepsilon)$ is some function such that $o(\varepsilon) -> 0$ as $\varepsilon -> 0$. This gives us that
$$
\forall i,j \in S, \varepsilon>0 \quad \pr (X(t+\varepsilon)=j|X(t)=i) = (e^{\varepsilon Q})_{ij},
$$
where $(e^{\varepsilon Q})_{ij}$ is the $i,j^{th}$ element of the matrix exponential of $\varepsilon Q$. With the initial probability vector $\bm{\delta}$ defined as before, we can uniquely define any CTMC with the triple $(S,\bm{\delta},Q)$. $Q$'s row sums are always 0, with all off-diagonal elements being non-negative, and all diagonal elements being non-positive, i.e.

\begin{align*}
\forall i,j \in S, i \neq j => q_{ij} \geqslant 0 \\
\forall i \in S, q_{ii} = -\sum_{j \neq i} q_{ij}
\end{align*}

\subsection{An Intuitive Interpretation}

Whilst the above defines the Markov Process, it fails to describe it in any intuitive way. Before attempting to use them, it is important to be able to deal with both the continuous and discrete-time Markov Chains in an intuitive way. My personal preferred method is to use edge-weighted directed graphs \cite{mwgraph}. Each state in $S$ is given a node in the graph, and for each $i,j \in S$ the edge $(i,j)$ is given weight $\pi_{ij}$ in the discrete case and $q_{ij}$ in the continuous case. Generally, in the continuous case we don't draw transition rates from each node to itself, since these can be inferred from the node's surrounding edges.

As an example, let's use the following definitions, with $\bm{\delta}$ and $\Pi$ indexed in the order the elements of $S$ are written;

\begin{align*}
S &= \{\mbox{John Paul II}, \mbox{Adrian I}\}\\
\bm{\delta} &= \left(1,0\right)\\
\Pi &=
\bordermatrix{      & JP2&A1 \cr
                JP2 & 0.3 &  0.7 \cr
                A1  & 0.9 &  0.1 
			}
\end{align*}

This defines a Discrete Time Markov Chain with the graph in Figure \ref{popes}:

\begin{figure}[h]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.5cm]
  \node[state]         (r)                     {JP2};
  \node[state]         (b)  [right of = r]     {A1};

  \path  (r) edge[loop left]   node {$0.3$} (r)
         ([yshift=1ex]r.east) edge node {$0.7$} ([yshift=1ex]b.west)

         (b) edge[loop right]   node {$0.1$} (b)
         ([yshift=-1ex]b.west)edge node {$0.9$} ([yshift=-1ex]r.east)

  ;
\end{tikzpicture}
\caption{A DTMC's graph - the initial probability vector may be represented by arrows entering from the outside, but this is not universal}\label{popes}
\end{figure}

We can then say that a DTMC will hop from node to node at each time step with probabilities defined by the weights of the edges between the current node and its neighbors, and can be simulated by algorithm \ref{algmc} \cite[\textsection 1.1]{mc_simulation}.
\begin{algorithm}
\SetAlgoLined
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}

\caption{A Simulation Algorithm for the generic Markov Chain}\label{algmc}
\Input{$(S,\bm{\delta},\Pi)$, a Markov Chain, and $T$, a maximum number of steps to simulate}
\Output{$\mathbf{x}$, a vector of states $x_i$, recording the sequence of states visited by the chain}

\Begin{
	$x_1 <- \sigma \quad wp \; \; \delta_\sigma$ \tcc{x1 takes on state sigma 
											with probability delta sigma}
	\For{$n <- 2$ \KwTo $T$}{
		$x_n <- \sigma \quad wp \; \; \pi_{x_{n-1},\sigma}$
		\tcc{xn takes on state sigma with the relevant probability}
	}
	\KwRet $\mathbf{x}$
}
\end{algorithm}
A CTMC is slightly more complex; on arriving in a state $i$, a random time $T \sim \mathrm{Exp}(-q_{ii})$ is generated. The CTMC will remain in state $i$ for $T$ units of time, then jump to state $j$ with probability $\frac{-q_{ij}}{q_{ii}}$. We can simulate a CTMC with algorithm \ref{algctmc}\cite[\textsection 1.3]{mc_simulation}.

\begin{algorithm}
\SetAlgoLined
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}

\caption{A Simulation Algorithm for the generic CTMC}\label{algctmc}
\Input{$(S,\bm{\delta},Q)$, a CTMC, and $T$, a maximum time for which to simulate the process}
\Output{$\mathbf{t}$, a vector of pairs $(t_i,x_i)$, recording the time and destination of the $i^{th}$ transition.}

\Begin{
	$t_0 <- 0$ \\
	$x_0 <- \sigma \quad wp \; \; \delta_\sigma$ \tcc{x0 takes on state sigma 
											with probability delta sigma}
	$n <- 0$\\
	\While{$t_n<T$}{
		$\tau <- Exp(-q_{x_nx_n})$ \tcc{tau takes on an exponentially distributed 
									random value}
		
		$n <- n+1$\\
		$t_n <- t_{n-1} + \tau$\\
		$x_n <- \sigma \quad wp \; \; \frac{-q_{x_n\sigma}}{q_{x_nx_n}}$
		\tcc{sn takes on state sigma with the given probability}
	}
	\KwRet $((t_0,x_0),...,(t_{n-1},x_{n-1}))$
}
\end{algorithm}
\clearpage
\subsection{The Poisson Process}

The simplest form of CTMC is the homogeneous Poisson process, where $S = \mathbb{N}$, $\delta_0=1$, and $\forall i \in S, q_{ii}=-\lambda, q_{i,i+1} = \lambda$. We call $\lambda$ the rate of this process. If $N(t)$ is a Poisson process, we then have that, for small $\varepsilon$

$$
\forall i \in \mathbb{N}, \pr (N(t+\varepsilon) = i+1 | N(t) = i) = \lambda \varepsilon + o(\varepsilon)
$$

Note that $N$ is the associated counting process in which $N(t)$ counts a number of events up to time $t$. $N$ is a stochastic process -- not a distribution or a random variable. The graph for this process is similarly simple, shown in Figure \ref{poisson_graph}.

\begin{figure}[h]
\centering
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3.5cm]
  \node[state]         (0)                     {0};
  \node[state]         (1)  [right of = 0]     {1};
  \node[state]         (2)  [right of = 1]     {2};
  \node[state]         (3)  [right of = 2]     {3};
  \node                (etc)[right of = 3, node distance = 2cm]     {};

  \path  (0) edge   node {$\lambda$} (1)
         (1) edge   node {$\lambda$} (2)
         (2) edge   node {$\lambda$} (3)
  ;
  \draw[dashed] (3) -- (etc);
\end{tikzpicture}
\caption{The graph of a Poisson process}\label{poisson_graph}
\end{figure}

Implicit from this, we see that the process can only increase -- once leaving a state, the process never returns, so we can define the emission times $t_n$, as $t_n = \min\{t : N(t)= n\}$. We can also define $\tau_n = t_n-t_{n-1}$, the inter-arrival times for the $n^{th}$ jump for $n \in \{1,2,...\}$. We refer to these as emissions and arrivals since a Poisson process generally models a counting process which records the times at which we observe events happening, eg the times at which radioactive particles are detected from a radioactive material. The most crucial property of the Poisson process for this work, implicit from algorithm \ref{algctmc}, is that

$$
\forall n \in \mathbb{N} \; \; \tau_n \sim \mathrm{Exp}(\lambda)
$$

That is, all inter-arrival times follow an Exponential distribution. A modification of the generic CTMC simulation algorithm can be made for simulating  a Poisson Process. Since it's only possible to jump from state $i$ to state $i+1$, we need not record the destinations of each jump; jump $i$ will always take us to state $i$. This is detailed in algorithm \ref{algpp}.

\begin{algorithm}
\SetAlgoLined
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}

\caption{A Simulation Algorithm for the Poisson Process}\label{algpp}
\Input{$\lambda$, a Poisson process rate and $T$, a maximum time for which to simulate}
\Output{$\mathbf{t}$, the emission times of a Poisson process of rate $\lambda$ terminating before time $T$}

\Begin{
	$t_0 <- 0$\\
	$n <- 0$\\
	\While{$t_n<T$}{
		$n <- n+1$\\
		$\tau_n <- \mathrm{Exp}(\lambda)$ \tcc{tau takes on an exponentially distributed 
									random value}
		$t_n <- t_{n-1} + \tau$
	}
	\KwRet $(t_0,...,t_{n-1})$
}
\end{algorithm}

We can also define an inhomogeneous Poisson process. Everything remains from before, except rather than having a rate parameter $\lambda \in \mathbb{R}^{+}$, we have a rate function $\bm{\lambda} : \mathbb{R}^{+} -> \mathbb{R}^{+}$, where, for sufficiently small $\varepsilon$,

$$
\forall i \in \mathbb{N}, t \in \mathbb{R}^{+}, \pr (N(t+\varepsilon) = i+1 | N(t) = i) = \bm{\lambda}(t) \varepsilon + o(\varepsilon)
$$

This process is no longer homogeneous, however it remains a powerful and more generic tool. Simulating one of these is, however, less simple than for the homogeneous case. The algorithm for doing so is based on Bernoulli thinning \cite{thinning} and is equivalent to algorithm \ref{algthin}.

\begin{algorithm}
\SetAlgoLined
\SetKwInOut{Input}{input}
\SetKwInOut{Output}{output}

\caption{A Thinning Algorithm for Poisson Processes}\label{algthin}
\Input{$\bm{\lambda} : [0,T] -> [0,\lambda_{max}]$, a desired rate function for the resulting Poisson process, and $\mathbf{t}$, the emission times of a Poisson Process of rate no greater than $\lambda_{max}$, terminating before time $T$, indexed from 1 to $n$}
\Output{$\mathbf{t'}$, the emission times for a single realisation of an inhomogeneous Poisson Process with rate function $\lambda$}

\Begin{
	$j <- 0$
	\For{$i <- 1$ \KwTo $n$}{
		$r <- U(0,1)$ \tcc{r takes on a uniformly distributed random value in 	
							[0,1]}
		\If{$r < \frac{\bm{\lambda}(t_i)}{\lambda_{max}}$}{
			$t'_j <- t_i$ \\
			$j <- j+1$
		}
	}
	\KwRet $\mathbf{t'}$
}
\end{algorithm}

To simulate an inhomogeneous Poisson process, we first simulate a homogeneous Poisson process of constant rate $\lambda_{max}$ with algorithm \ref{algpp}, where $\lambda_{max}$ is some upper bound on the inhomogeneous rate function $\bm{\lambda}$, and then keep each emission at time $t_i$ with a probability proportional to the inhomogeneous rate function $\bm{\lambda}(t_i)$ using algorithm \ref{algthin}.

\section{Hidden Markov Models}

The Hidden Markov Model is an extension of the Markov Chain, in which the chain itself may not be fully observable. With discrete time and state space, rather than having a directly observable Markov Chain, it is assumed that the process has an underlying unobservable Markov Chain. An observation space, $Y$, is defined, along with a set of probability densities on $Y$, $\{p_s : s \in S\}$. At time $i$, $x_i$ represents the state of the underlying Markov Chain. A single observation, $y_i$, will be emitted by the HMM according to the distribution defined by $p_{x_i}$, before jumping into a new state and making another emission.

The traditional example is that of the ``unfair casino". Imagine the dealer at the casino has two six-sided dice. Die one is fair, but die two is weighted such that it never shows a 6, and shows the numbers 1 to 5 with equal probabilities. The dealer will clandestinely swap the dice with probability $0.1$ before rolling. He selects his first die uniformly. We can represent this as follows;

\begin{align*}
S &= \{1,2\}\\
\bm{\delta} &= (0.5,0.5)\\
\Pi &= 
\left(
	\begin{matrix}
	0.9 & 0.1 \\
	0.1 & 0.9
	\end{matrix}
\right)\\
Y &= \{1,2,3,4,5,6\}\\
y \in Y => p_1(y) &= \frac{1}{6}\\
y \in Y \setminus \{6\} => p_2(y) &= \frac{1}{5}
\end{align*}

We can only observe the results of the dice rolls, but what we're interested in is the sequence of states entered by the model. Which die is the dealer using at which time? Further to this, suppose we didn't know the above parameters in advance. We may also want to estimate them from our observations in some way.

\section{The Markov-Modulated Poisson Process}\label{mmppdef}

The Markov-Modulated Poisson Process (MMPP) is a particular type of Hidden Markov Model. We assume that, underlying some inhomogeneous Poisson process, there is a CTMC. Each state is tied to a fixed Poisson Process rate, so $S = \{\lambda_1,...,\lambda_{|S|}\}$. The instantaneous rate of the Poisson process is then defined by the state in which the underlying CTMC resides. We can define such a process by letting $\mathbf{t} = ((t_0,s_0),...,(t_n,s_n))$ be a realisation of the CTMC as generated by algorithm \ref{algctmc}, $[n-1] = \{1,2,...,n-1\}$, and $\bm{\lambda} : [0,T]->S$ be the step function generated by simulating the underlying CTMC, i.e.;

$$
\bm{\lambda}(\tau) = 
\begin{cases}
	s_i & \mbox{for} \quad \tau \in [t_i,t_{i+1}), \quad i \in [n-1]\\
	s_n & \mbox{for} \quad \tau \in [t_n,T]
\end{cases}
$$

We then have that an inhomogeneous Poisson process of rate $\bm{\lambda}$ is a single realisation of this MMPP. In order to generate multiple realisations, it is necessary for us to produce a new rate function $\bm{\lambda}$ from the underlying CTMC for each realisation.

Though this is correct, I personally find that it is simpler to imagine an MMPP as the concatenation of a series of Poisson processes, each generated within a particular state of the underlying CTMC. The underlying CTMC enters state $s_1$ at time $t_1$. It generates emissions as a Poisson Process of rate $s_1$ for $t_2-t_1$ time units. It then stops acting like this Poisson process, and starts generating emissions as a Poisson process of rate $s_2$ for $t_3-t_2$ time units.

It's not immediately obvious that this intuition matches the definition, so it's worth arguing that this is the case.

Let $\bm{\lambda}_1: [0,T_1) -> \mathbb{R}^{+}$ and $\bm{\lambda}_2:[0,T_2) -> \mathbb{R}^{+}$ be two rate functions. Let $T = T_1 + T_2$, and define $\bm{\lambda}$, the concatenation $\bm{\lambda} = \bm{\lambda}_1 || \bm{\lambda}_2$, as;

$$
\bm{\lambda}(\tau) = 
\begin{cases}
	\bm{\lambda}_1(\tau) & \mbox{for} \quad \tau \in [0,T_1)\\
	\bm{\lambda}_2(\tau-T_1) & \mbox{for} \quad \tau \in [T_1,T)
\end{cases}
$$

Let $\{N_1(t) : t \in [0,T_1)\}$ be a Poisson process of rate $\bm{\lambda}_1$, similarly for $N_2$ and $N$, and let $\widetilde{N}$ be defined thus;

$$
\widetilde{N}(\tau) = 
\begin{cases}
	N_1(\tau) & \mbox{for} \quad \tau \in [0,T_1)\\
	N_1(T_1) + N_2(\tau-T_1) & \mbox{for} \quad \tau \in [T_1,T)
\end{cases}
$$

$\widetilde{N}$ represents my intuition that an MMPP can be formed by gluing together emissions from shorter fixed rate Poisson processes, and $N$ represents the actual definition given above. If we can show that $\widetilde{N}$ and $N$ are identically distributed, then we can inductively apply this result to arbitrarily long concatenations, and hence to the generic MMPP.

Let $\tau \in [0,T_1)$ and $\varepsilon>0$ be such that $\tau + \varepsilon < T_1$. We then have that

\begin{align*}
\pr (\widetilde{N}(\tau+\varepsilon) - \widetilde{N}(\tau) = 1)
	&= \pr (N_1(\tau+\varepsilon) - N_1(\tau) = 1) \\
	&= \bm{\lambda}_1(\tau)\varepsilon + o(\varepsilon) \\
	&= \bm{\lambda}(\tau)\varepsilon + o(\varepsilon)
\end{align*} 

So $\widetilde{N}$ is identical to a Poisson process of rate $\bm{\lambda}$ within the range $[0,T_1)$.

Now let $\tau \in [T_1,T)$ and $\varepsilon>0$ be such that $\tau + \varepsilon < T$. We have that

\begin{align*}
\pr (\widetilde{N}(\tau+\varepsilon) - \widetilde{N}(\tau) = 1)
	&= \pr (N_2(\tau-T_1+\varepsilon) + N_1(T_1) - N_2(\tau-T_1) - N_1(T_1)= 1)\\
	&= \pr (N_2(\tau-T_1+\varepsilon) - N_2(\tau-T_1) = 1)\\
	&= \bm{\lambda}_2(\tau-T_1)\varepsilon + o(\varepsilon) \\
	&= \bm{\lambda}(\tau)\varepsilon + o(\varepsilon)
\end{align*}

So $\widetilde{N}$ is identical to a Poisson process of rate $\bm{\lambda}$ within the range $[T_1,T)$. We need not deal with the case where $\tau$ and $\tau+\varepsilon$ are either side of $T_1$, since we can always choose an $\varepsilon$ small enough such that this is not the case -- these probabilities assume that $\varepsilon$ is close to 0. So we have that $\widetilde{N}$ is distributed identically to $N$.

So concatenating two Poisson processes of rates $\bm{\lambda}_1$ and $\bm{\lambda}_2$ produces a new Poisson process of rate $\bm{\lambda}_1||\bm{\lambda}_2$, and it follows from simple induction that an MMPP can be thought of as the concatenation of multiple bounded-length homogeneous Poisson processes of rates determined by the state of the underlying CTMC, and time bounds determined by the length of time spent in each state of the underlying CTMC.
